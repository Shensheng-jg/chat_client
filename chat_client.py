import os
import re
import io
import base64
from typing import List, Optional
import gradio as gr
from openai import OpenAI
from dotenv import load_dotenv
from PIL import Image

load_dotenv()

# ===== é»˜è®¤é…ç½®ï¼ˆå¯ç”¨ .env è¦†ç›–ï¼‰=====
DEFAULT_BASE_URL = os.getenv("LLM_BASE_URL", "https://api.openai-proxy.org/v1")
DEFAULT_API_KEY  = os.getenv("LLM_API_KEY", "")
DEFAULT_MODEL    = os.getenv("LLM_MODEL", "gemini-1.5-pro")
DEFAULT_SYSTEM   = os.getenv("LLM_SYSTEM_PROMPT", "You are a helpful AI assistant.")
DEFAULT_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))

# ===== å·¥å…·å‡½æ•° =====
def _normalize_base_url(url: str) -> str:
    if not url:
        return url
    u = url.rstrip("/")
    if re.search(r"/v\d+([a-zA-Z].*)?$", u):
        return u
    return u + "/v1"

def _make_client(api_key: str, base_url: str) -> OpenAI:
    if not api_key:
        raise ValueError("è¯·å¡«å†™ API Key")
    if not base_url:
        raise ValueError("è¯·å¡«å†™ Base URL")
    nb = _normalize_base_url(base_url)
    return OpenAI(api_key=api_key, base_url=nb)

def _ensure_pairs(hist):
    fixed = []
    for m in hist or []:
        if isinstance(m, tuple):
            m = list(m)
        if not (isinstance(m, list) and len(m) == 2):
            continue
        u, a = m
        u = "" if u is None else str(u)
        a = "" if a is None else str(a)
        fixed.append([u, a])
    return fixed

def _compress_to_data_url(img: Image.Image, fmt="JPEG", max_side=1600, quality=85) -> str:
    # ç¼©æ”¾åˆ°è¾ƒå°è¾¹ç•Œï¼Œé™ä½ token / è´¹ç”¨
    w, h = img.size
    if max(w, h) > max_side:
        if w >= h:
            nw, nh = max_side, int(h * (max_side / w))
        else:
            nh, nw = max_side, int(w * (max_side / h))
        img = img.resize((nw, nh))
    if img.mode in ("RGBA", "P"):
        img = img.convert("RGB")
    buf = io.BytesIO()
    img.save(buf, fmt, quality=quality, optimize=True)
    b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    mime = "image/jpeg" if fmt.upper() == "JPEG" else "image/png"
    return f"data:{mime};base64,{b64}"

def _filepath_to_data_url(path: str) -> Optional[str]:
    try:
        with Image.open(path) as im:
            return _compress_to_data_url(im, fmt="JPEG")
    except Exception:
        return None

def _build_user_content(user_text: str,
                        file_paths: Optional[List[str]],
                        url_text: Optional[str]):
    """
    è¿”å› messages ä¸­ user çš„ contentï¼ˆlist[parts]ï¼‰ï¼Œä»¥åŠç”¨äºå¯è§†åŒ–çš„å ä½è¯´æ˜ã€‚
    """
    parts = []
    if user_text:
        parts.append({"type": "text", "text": user_text})

    # URLï¼ˆå¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ªï¼‰
    url_list = []
    if url_text:
        for line in url_text.splitlines():
            u = line.strip()
            if u:
                url_list.append(u)

    for u in url_list:
        parts.append({"type": "image_url", "image_url": {"url": u, "detail": "high"}})

    # æœ¬åœ°æ–‡ä»¶ â†’ data URL
    count_files = 0
    if file_paths:
        for p in file_paths:
            data_url = _filepath_to_data_url(p)
            if data_url:
                parts.append({"type": "image_url", "image_url": {"url": data_url, "detail": "high"}})
                count_files += 1

    # å¯è§†åŒ–å ä½æ–‡æ¡ˆï¼ˆä¸æŠŠ base64 å¡è¿›èŠå¤©çª—å£ï¼Œé¿å… UI è¿‡å¤§ï¼‰
    attach_note = ""
    total_imgs = len(url_list) + count_files
    if total_imgs > 0:
        attach_note = f"\n\nğŸ–¼ï¸ å·²é™„åŠ  {total_imgs} å¼ å›¾ç‰‡ï¼ˆ{count_files} æœ¬åœ° / {len(url_list)} URLï¼‰"

    visible_user_text = (user_text or "").strip()
    visible_user_text = visible_user_text + attach_note

    return parts, visible_user_text

# ===== æ ¸å¿ƒï¼šæµå¼ + å…œåº• =====
def stream_chat(api_key, base_url, model, system_prompt, temperature,
                history, user_msg, image_files, image_urls):
    """
    ç”Ÿæˆå™¨ï¼šæ¯æ¬¡ yield è¿”å› (chat_history, state) ä¸¤ä¸ªè¾“å‡ºã€‚
    history: list[[user, assistant], ...]
    image_files: list[str] (gr.Files è¿”å›æ–‡ä»¶è·¯å¾„åˆ—è¡¨)
    image_urls: str (å¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ª URL)
    """
    history = _ensure_pairs(history)
    client = _make_client(api_key, base_url)

    # å‡†å¤‡ user æ¶ˆæ¯çš„ contentï¼ˆæ–‡å­— + å›¾ç‰‡ï¼‰
    file_paths = []
    if image_files:
        # gr.Files åœ¨ Gradio v4 ä¸­ä¼ å…¥çš„æ˜¯å«æœ‰ .name å±æ€§çš„å¯¹è±¡æˆ–çº¯è·¯å¾„ï¼›éƒ½æŒ‰è·¯å¾„æ”¶é›†
        for f in image_files:
            if isinstance(f, str):
                file_paths.append(f)
            else:
                # TemporaryFile or UploadFile-like
                path = getattr(f, "name", None) or getattr(f, "path", None)
                if path:
                    file_paths.append(path)

    user_parts, visible_user = _build_user_content(user_msg, file_paths, image_urls)

    if len(user_parts) == 0:
        # æ—¢æ²¡æœ‰æ–‡æœ¬ä¹Ÿæ²¡æœ‰å›¾ç‰‡ï¼Œç›´æ¥æç¤º
        history.append(["ï¼ˆç©ºæ¶ˆæ¯ï¼‰", "âš ï¸ è¯·è¾“å…¥æ–‡æœ¬æˆ–é™„åŠ å›¾ç‰‡/URL"])
        yield _ensure_pairs(history), _ensure_pairs(history)
        return

    # ç»„è£…å®Œæ•´ messages
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    for u, a in history:
        if u:
            messages.append({"role": "user", "content": u})
        if a:
            messages.append({"role": "assistant", "content": a})
    messages.append({"role": "user", "content": user_parts})

    # å…ˆåœ¨ UI å ä½
    history.append([visible_user, ""])
    yield _ensure_pairs(history), _ensure_pairs(history)

    # é¦–é€‰ï¼šchat.completions æµå¼
    partial = ""
    try:
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=True,
        )
        for chunk in stream:
            delta = getattr(chunk.choices[0], "delta", None)
            if delta and getattr(delta, "content", None):
                partial += delta.content
                history[-1][1] = partial
                yield _ensure_pairs(history), _ensure_pairs(history)
        return
    except Exception as e1:
        err1 = f"{type(e1).__name__}: {e1}"

    # å…œåº•ï¼šresponsesï¼ˆå¤šæ•°ä»£ç†å¯¹å›¾ç‰‡ä¸ä¸€å®šå…¼å®¹ï¼Œæ­¤å¤„ä»…ä¿æ–‡æœ¬ï¼‰
    try:
        # æŠŠ messages å‹æˆçº¯æ–‡æœ¬åšé™çº§
        def _msgs_to_text(msgs):
            lines = []
            for m in msgs:
                role = m.get("role", "user")
                content = m.get("content", "")
                if isinstance(content, list):
                    # ä»…æ‹¼æ¥æ–‡æœ¬éƒ¨åˆ†
                    texts = []
                    for part in content:
                        if part.get("type") in ("text", "input_text"):
                            texts.append(part.get("text") or part.get("content") or "")
                    content = "\n".join(texts)
                lines.append(f"{role.upper()}: {content}")
            return "\n".join(lines)

        r = client.responses.create(
            model=model,
            input=_msgs_to_text(messages),
            temperature=temperature,
        )
        reply = getattr(r, "output_text", None) or str(r)
        history[-1][1] = reply
        yield _ensure_pairs(history), _ensure_pairs(history)
    except Exception as e2:
        history[-1][1] = f"âš ï¸ è¯·æ±‚å‡ºé”™ï¼ˆå·²å°è¯• chat.completions ä¸ responsesï¼‰ï¼š\n1) {err1}\n2) {type(e2).__name__}: {e2}"
        yield _ensure_pairs(history), _ensure_pairs(history)

def clear_all():
    empty = []
    return empty, empty

# ===== Gradio UI =====
with gr.Blocks(title="LLM Client (OpenAI-Compatible)", theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ§  LLM Client Â· OpenAI-Compatibleï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰")

    with gr.Row():
        base_url = gr.Textbox(label="Base URL", value=DEFAULT_BASE_URL, placeholder="å¦‚ï¼šhttps://api.openai-proxy.org/v1")
        api_key  = gr.Textbox(label="API Key", value=DEFAULT_API_KEY, type="password", placeholder="ä½ çš„å¯†é’¥")

    with gr.Row():
        model = gr.Textbox(label="Model", value=DEFAULT_MODEL, placeholder="å¦‚ï¼šgemini-1.5-pro / gpt-4o-mini")
        temperature = gr.Slider(0.0, 1.5, value=DEFAULT_TEMPERATURE, step=0.1, label="Temperature")

    system_prompt = gr.Textbox(label="System Promptï¼ˆå¯é€‰ï¼‰", value=DEFAULT_SYSTEM, lines=2)

    # æ–°å¢ï¼šå›¾ç‰‡è¾“å…¥ï¼ˆæœ¬åœ°ä¸ URLï¼‰
    with gr.Accordion("é™„åŠ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰", open=False):
        image_files = gr.Files(label="ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯å¤šé€‰ï¼‰", file_types=["image"], file_count="multiple")
        image_urls  = gr.Textbox(label="å›¾ç‰‡ URLï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œå¯å¤šè¡Œï¼‰", lines=3, placeholder="https://.../a.jpg\nhttps://.../b.png")

    chat = gr.Chatbot(height=420, avatar_images=(None, None))
    msg = gr.Textbox(placeholder="è¾“å…¥æ–‡æœ¬ï¼Œå¯é€‰é™„åŠ å›¾ç‰‡æˆ–å›¾ç‰‡URL", label="Message")
    with gr.Row():
        send_btn = gr.Button("å‘é€", variant="primary")
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")

    state = gr.State([])

    inputs = [api_key, base_url, model, system_prompt, temperature, state, msg, image_files, image_urls]
    outputs = [chat, state]

    send_btn.click(stream_chat, inputs=inputs, outputs=outputs)
    msg.submit(stream_chat, inputs=inputs, outputs=outputs)
    clear_btn.click(clear_all, None, outputs)

if __name__ == "__main__":
    demo.queue().launch(server_name="0.0.0.0", server_port=7860, show_error=True)
